{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.common import load_file_lists\n",
    "from models.preprocessor import OpenL3PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def create_features(input_data):\n",
    "    try:\n",
    "        X_np = np.load(os.path.join(features_path, \"X.npy\"), allow_pickle=True)\n",
    "        Y_np = np.load(os.path.join(features_path, \"Y.npy\"), allow_pickle=True)\n",
    "    except OSError:\n",
    "        X = []\n",
    "        Y = []\n",
    "        for idx, filename in input_data:\n",
    "            filename = os.path.join(\"../data/mtat/emb\", str(pathlib.Path(filename).with_suffix(\".npy\")))\n",
    "            file_data = np.load(filename, allow_pickle=True).flatten()\n",
    "            X.append(file_data)\n",
    "            Y.append(binary[int(idx)])\n",
    "        X_np = np.array(X)\n",
    "        Y_np = np.array(Y)\n",
    "        np.save(os.path.join(features_path, \"X.npy\"), X_np)\n",
    "        np.save(os.path.join(features_path, \"y.npy\"), Y_np)\n",
    "    return X_np, Y_np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 123456\n"
     ]
    }
   ],
   "source": [
    "set_seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "dataset_name = \"mtat-20\"\n",
    "split_path = \"../split\"\n",
    "features_path = \"../data/mtat/features/\"\n",
    "\n",
    "split_path = os.path.join(split_path, dataset_name)\n",
    "features_path = os.path.join(features_path, dataset_name)\n",
    "os.makedirs(split_path, exist_ok=True)\n",
    "os.makedirs(features_path, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = OpenL3PreProcessor(input_path=\"../data/mtat/mp3\",\n",
    "#                        output_path=\"../data/mtat/emb\",\n",
    "#                        suffix=\"npy\")\n",
    "# # print(load_file_lists([\"../split/mtat/train.npy\", \"../split/mtat/valid.npy\", \"../split/mtat/test.npy\"])[:, 1])\n",
    "data = load_file_lists([\n",
    "    os.path.join(split_path, \"train.npy\"),\n",
    "    os.path.join(split_path, \"valid.npy\"),\n",
    "    os.path.join(split_path, \"test.npy\")\n",
    "])\n",
    "# p.run(files=data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = {row[0]: row[1:] for row in np.load(os.path.join(split_path, \"binary.npy\"), allow_pickle=True)}\n",
    "tags = np.load(os.path.join(split_path, \"tags.npy\"), allow_pickle=True)\n",
    "X_np, Y_np = create_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_np, Y_np, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.92      0.62       612\n",
      "           1       0.59      0.78      0.67       785\n",
      "           2       0.07      0.61      0.13       101\n",
      "           3       0.54      0.83      0.65       487\n",
      "           4       0.14      0.60      0.22       149\n",
      "           5       0.11      0.80      0.19        88\n",
      "           6       0.07      0.57      0.13        81\n",
      "           7       0.57      0.98      0.72       353\n",
      "           8       0.08      0.66      0.14        68\n",
      "           9       0.35      1.00      0.51       184\n",
      "          10       0.32      0.90      0.47       176\n",
      "          11       0.15      0.71      0.24       105\n",
      "          12       0.36      0.78      0.49       195\n",
      "          13       0.01      0.75      0.01         4\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.01      1.00      0.02         4\n",
      "          17       0.39      0.85      0.53       154\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.25      0.83      0.39      3547\n",
      "   macro avg       0.21      0.64      0.29      3547\n",
      "weighted avg       0.43      0.83      0.55      3547\n",
      " samples avg       0.31      0.46      0.34      3547\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.51      0.52      1222\n",
      "           1       0.55      0.53      0.54      1080\n",
      "           2       0.28      0.28      0.28       894\n",
      "           3       0.54      0.55      0.55       730\n",
      "           4       0.37      0.36      0.37       687\n",
      "           5       0.29      0.30      0.29       636\n",
      "           6       0.32      0.31      0.32       648\n",
      "           7       0.56      0.57      0.56       596\n",
      "           8       0.26      0.26      0.26       566\n",
      "           9       0.39      0.43      0.41       478\n",
      "          10       0.39      0.40      0.39       484\n",
      "          11       0.29      0.35      0.32       433\n",
      "          12       0.43      0.40      0.41       448\n",
      "          13       0.23      0.21      0.22       495\n",
      "          14       0.18      0.18      0.18       411\n",
      "          15       0.20      0.19      0.19       384\n",
      "          16       0.20      0.20      0.20       351\n",
      "          17       0.52      0.51      0.51       345\n",
      "          18       0.20      0.20      0.20       333\n",
      "          19       0.15      0.14      0.14       342\n",
      "\n",
      "   micro avg       0.37      0.37      0.37     11563\n",
      "   macro avg       0.34      0.34      0.34     11563\n",
      "weighted avg       0.37      0.37      0.37     11563\n",
      " samples avg       0.41      0.40      0.36     11563\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72       987\n",
      "           1       0.73      0.70      0.71      1091\n",
      "           2       0.26      0.50      0.34       456\n",
      "           3       0.72      0.72      0.72       749\n",
      "           4       0.40      0.53      0.46       494\n",
      "           5       0.41      0.54      0.47       506\n",
      "           6       0.31      0.51      0.39       379\n",
      "           7       0.74      0.88      0.81       510\n",
      "           8       0.40      0.43      0.41       540\n",
      "           9       0.56      0.79      0.66       383\n",
      "          10       0.63      0.68      0.65       464\n",
      "          11       0.40      0.47      0.43       435\n",
      "          12       0.62      0.70      0.66       378\n",
      "          13       0.12      0.47      0.19       115\n",
      "          14       0.17      0.42      0.25       178\n",
      "          15       0.28      0.65      0.39       161\n",
      "          16       0.52      0.65      0.58       274\n",
      "          17       0.81      0.81      0.81       338\n",
      "          18       0.25      0.49      0.33       164\n",
      "          19       0.12      0.44      0.19        89\n",
      "\n",
      "   micro avg       0.49      0.65      0.55      8691\n",
      "   macro avg       0.46      0.61      0.51      8691\n",
      "weighted avg       0.54      0.65      0.58      8691\n",
      " samples avg       0.55      0.64      0.54      8691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "model = KNeighborsClassifier(),\n",
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "dataset_name = \"mtat-10\"\n",
    "split_path = \"../split\"\n",
    "features_path = \"../data/mtat/features/\"\n",
    "\n",
    "split_path = os.path.join(split_path, dataset_name)\n",
    "features_path = os.path.join(features_path, dataset_name)\n",
    "os.makedirs(split_path, exist_ok=True)\n",
    "os.makedirs(features_path, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# p = OpenL3PreProcessor(input_path=\"../data/mtat/mp3\",\n",
    "#                        output_path=\"../data/mtat/emb\",\n",
    "#                        suffix=\"npy\")\n",
    "# # print(load_file_lists([\"../split/mtat/train.npy\", \"../split/mtat/valid.npy\", \"../split/mtat/test.npy\"])[:, 1])\n",
    "data = load_file_lists([\n",
    "    os.path.join(split_path, \"train.npy\"),\n",
    "    os.path.join(split_path, \"valid.npy\"),\n",
    "    os.path.join(split_path, \"test.npy\")\n",
    "])\n",
    "# p.run(files=data[:, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "binary = {row[0]: row[1:] for row in np.load(os.path.join(split_path, \"binary.npy\"), allow_pickle=True)}\n",
    "tags = np.load(os.path.join(split_path, \"tags.npy\"), allow_pickle=True)\n",
    "X_np, Y_np = create_features(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_np, Y_np, random_state=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.91      0.73       787\n",
      "           1       0.73      0.80      0.76      1001\n",
      "           2       0.22      0.76      0.34       259\n",
      "           3       0.59      0.82      0.69       538\n",
      "           4       0.25      0.69      0.37       246\n",
      "           5       0.17      0.80      0.28       137\n",
      "           6       0.13      0.58      0.21       137\n",
      "           7       0.67      0.97      0.79       382\n",
      "           8       0.10      0.64      0.17        89\n",
      "           9       0.32      0.99      0.49       172\n",
      "\n",
      "   micro avg       0.41      0.83      0.55      3748\n",
      "   macro avg       0.38      0.79      0.48      3748\n",
      "weighted avg       0.54      0.83      0.63      3748\n",
      " samples avg       0.46      0.60      0.49      3748\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59      1192\n",
      "           1       0.63      0.62      0.63      1096\n",
      "           2       0.34      0.36      0.35       869\n",
      "           3       0.59      0.57      0.58       769\n",
      "           4       0.39      0.39      0.39       675\n",
      "           5       0.35      0.34      0.35       661\n",
      "           6       0.34      0.33      0.34       650\n",
      "           7       0.65      0.59      0.61       610\n",
      "           8       0.28      0.28      0.28       595\n",
      "           9       0.44      0.44      0.44       529\n",
      "\n",
      "   micro avg       0.48      0.47      0.47      7646\n",
      "   macro avg       0.46      0.45      0.46      7646\n",
      "weighted avg       0.48      0.47      0.47      7646\n",
      " samples avg       0.51      0.50      0.46      7646\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77      1043\n",
      "           1       0.80      0.75      0.77      1166\n",
      "           2       0.41      0.64      0.50       583\n",
      "           3       0.77      0.74      0.75       771\n",
      "           4       0.51      0.59      0.55       592\n",
      "           5       0.45      0.58      0.50       492\n",
      "           6       0.40      0.54      0.46       461\n",
      "           7       0.79      0.87      0.83       499\n",
      "           8       0.44      0.46      0.45       562\n",
      "           9       0.54      0.83      0.66       347\n",
      "\n",
      "   micro avg       0.60      0.70      0.65      6516\n",
      "   macro avg       0.58      0.68      0.62      6516\n",
      "weighted avg       0.62      0.70      0.65      6516\n",
      " samples avg       0.65      0.70      0.63      6516\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.90      0.62       609\n",
      "           1       0.54      0.74      0.62       759\n",
      "           2       0.03      0.63      0.05        38\n",
      "           3       0.50      0.80      0.61       464\n",
      "           4       0.12      0.68      0.21       122\n",
      "           5       0.07      0.84      0.14        57\n",
      "           6       0.04      0.56      0.08        52\n",
      "           7       0.56      0.96      0.70       331\n",
      "           8       0.06      0.67      0.12        54\n",
      "           9       0.32      0.99      0.49       169\n",
      "          10       0.25      0.79      0.38       164\n",
      "          11       0.14      0.65      0.23       105\n",
      "          12       0.31      0.81      0.45       170\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.02      0.75      0.03         8\n",
      "          17       0.23      0.83      0.36        84\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.50      0.92      0.65       145\n",
      "          23       0.00      0.50      0.01         2\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.02      1.00      0.04         5\n",
      "          26       0.00      0.00      0.00         0\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.00      0.00      0.00         0\n",
      "          31       0.01      0.75      0.03         4\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.00      0.00      0.00         0\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.48      0.87      0.61        90\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.00      0.00      0.00         0\n",
      "          38       0.03      0.36      0.05        11\n",
      "          39       0.00      0.00      0.00         0\n",
      "          40       0.00      0.00      0.00         0\n",
      "          41       0.00      0.00      0.00         0\n",
      "          42       0.00      0.00      0.00         0\n",
      "          43       0.01      1.00      0.03         2\n",
      "          44       0.00      0.00      0.00         0\n",
      "          45       0.00      0.00      0.00         0\n",
      "          46       0.00      0.00      0.00         0\n",
      "          47       0.00      0.00      0.00         0\n",
      "          48       0.04      0.50      0.08        10\n",
      "          49       0.25      0.64      0.36        42\n",
      "\n",
      "   micro avg       0.16      0.82      0.27      3498\n",
      "   macro avg       0.10      0.36      0.14      3498\n",
      "weighted avg       0.41      0.82      0.53      3498\n",
      " samples avg       0.22      0.41      0.26      3498\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.52      0.54      1242\n",
      "           1       0.54      0.50      0.52      1126\n",
      "           2       0.28      0.29      0.28       862\n",
      "           3       0.49      0.53      0.51       692\n",
      "           4       0.34      0.33      0.33       697\n",
      "           5       0.26      0.27      0.27       613\n",
      "           6       0.30      0.33      0.31       593\n",
      "           7       0.56      0.57      0.56       568\n",
      "           8       0.23      0.24      0.23       547\n",
      "           9       0.36      0.37      0.37       506\n",
      "          10       0.37      0.37      0.37       512\n",
      "          11       0.34      0.35      0.34       467\n",
      "          12       0.43      0.41      0.42       469\n",
      "          13       0.21      0.20      0.20       448\n",
      "          14       0.14      0.18      0.16       382\n",
      "          15       0.19      0.20      0.19       358\n",
      "          16       0.17      0.20      0.19       288\n",
      "          17       0.44      0.43      0.44       310\n",
      "          18       0.15      0.15      0.15       312\n",
      "          19       0.11      0.10      0.11       325\n",
      "          20       0.14      0.14      0.14       325\n",
      "          21       0.07      0.07      0.07       279\n",
      "          22       0.48      0.47      0.48       273\n",
      "          23       0.16      0.19      0.17       231\n",
      "          24       0.16      0.16      0.16       266\n",
      "          25       0.22      0.26      0.24       215\n",
      "          26       0.12      0.11      0.11       241\n",
      "          27       0.18      0.15      0.17       272\n",
      "          28       0.06      0.06      0.06       225\n",
      "          29       0.20      0.18      0.19       283\n",
      "          30       0.08      0.08      0.08       250\n",
      "          31       0.18      0.21      0.20       203\n",
      "          32       0.12      0.13      0.13       183\n",
      "          33       0.10      0.10      0.10       210\n",
      "          34       0.08      0.08      0.08       165\n",
      "          35       0.48      0.46      0.47       168\n",
      "          36       0.07      0.06      0.06       162\n",
      "          37       0.06      0.07      0.06       148\n",
      "          38       0.29      0.24      0.26       179\n",
      "          39       0.09      0.09      0.09       158\n",
      "          40       0.07      0.08      0.08       165\n",
      "          41       0.10      0.09      0.09       161\n",
      "          42       0.10      0.13      0.11       132\n",
      "          43       0.20      0.22      0.21       129\n",
      "          44       0.03      0.03      0.03       135\n",
      "          45       0.03      0.04      0.04       122\n",
      "          46       0.11      0.15      0.13       112\n",
      "          47       0.05      0.06      0.05       122\n",
      "          48       0.29      0.33      0.31       106\n",
      "          49       0.36      0.35      0.36       110\n",
      "\n",
      "   micro avg       0.29      0.30      0.29     17047\n",
      "   macro avg       0.22      0.23      0.22     17047\n",
      "weighted avg       0.30      0.30      0.29     17047\n",
      " samples avg       0.33      0.34      0.29     17047\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71       990\n",
      "           1       0.70      0.67      0.68      1093\n",
      "           2       0.24      0.48      0.32       451\n",
      "           3       0.71      0.69      0.70       761\n",
      "           4       0.42      0.54      0.47       521\n",
      "           5       0.38      0.56      0.45       438\n",
      "           6       0.30      0.55      0.39       360\n",
      "           7       0.73      0.81      0.77       516\n",
      "           8       0.38      0.44      0.41       489\n",
      "           9       0.54      0.80      0.64       351\n",
      "          10       0.56      0.61      0.58       474\n",
      "          11       0.38      0.45      0.41       411\n",
      "          12       0.63      0.73      0.67       385\n",
      "          13       0.16      0.53      0.25       135\n",
      "          14       0.14      0.36      0.21       187\n",
      "          15       0.28      0.59      0.38       172\n",
      "          16       0.56      0.64      0.60       294\n",
      "          17       0.74      0.74      0.74       306\n",
      "          18       0.18      0.44      0.25       128\n",
      "          19       0.08      0.34      0.13        70\n",
      "          20       0.05      0.31      0.08        48\n",
      "          21       0.00      0.03      0.01        30\n",
      "          22       0.71      0.82      0.76       233\n",
      "          23       0.27      0.41      0.32       177\n",
      "          24       0.10      0.38      0.16        68\n",
      "          25       0.63      0.74      0.68       213\n",
      "          26       0.15      0.39      0.22        90\n",
      "          27       0.10      0.39      0.16        61\n",
      "          28       0.00      0.10      0.01        10\n",
      "          29       0.15      0.39      0.21        95\n",
      "          30       0.03      0.20      0.06        45\n",
      "          31       0.58      0.80      0.67       171\n",
      "          32       0.08      0.25      0.12        63\n",
      "          33       0.07      0.39      0.12        33\n",
      "          34       0.05      0.21      0.08        38\n",
      "          35       0.71      0.75      0.73       156\n",
      "          36       0.00      0.00      0.00         8\n",
      "          37       0.03      0.16      0.05        32\n",
      "          38       0.32      0.37      0.34       128\n",
      "          39       0.05      0.25      0.09        32\n",
      "          40       0.05      0.45      0.09        20\n",
      "          41       0.07      0.26      0.11        38\n",
      "          42       0.28      0.69      0.40        70\n",
      "          43       0.57      0.78      0.66       101\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.01      0.12      0.01         8\n",
      "          46       0.27      0.55      0.36        73\n",
      "          47       0.03      0.50      0.05         8\n",
      "          48       0.52      0.53      0.53       116\n",
      "          49       0.61      0.60      0.61       111\n",
      "\n",
      "   micro avg       0.38      0.61      0.47     10809\n",
      "   macro avg       0.31      0.47      0.35     10809\n",
      "weighted avg       0.50      0.61      0.54     10809\n",
      " samples avg       0.46      0.59      0.46     10809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "model = KNeighborsClassifier(),\n",
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_pred, Y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "dataset_name = \"mtat\"\n",
    "split_path = \"../split\"\n",
    "features_path = \"../data/mtat/features/\"\n",
    "\n",
    "split_path = os.path.join(split_path, dataset_name)\n",
    "features_path = os.path.join(features_path, dataset_name)\n",
    "os.makedirs(split_path, exist_ok=True)\n",
    "os.makedirs(features_path, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# p = OpenL3PreProcessor(input_path=\"../data/mtat/mp3\",\n",
    "#                        output_path=\"../data/mtat/emb\",\n",
    "#                        suffix=\"npy\")\n",
    "# # print(load_file_lists([\"../split/mtat/train.npy\", \"../split/mtat/valid.npy\", \"../split/mtat/test.npy\"])[:, 1])\n",
    "data = load_file_lists([\n",
    "    os.path.join(split_path, \"train.npy\"),\n",
    "    os.path.join(split_path, \"valid.npy\"),\n",
    "    os.path.join(split_path, \"test.npy\")\n",
    "])\n",
    "# p.run(files=data[:, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "binary = {row[0]: row[1:] for row in np.load(os.path.join(split_path, \"binary.npy\"), allow_pickle=True)}\n",
    "tags = np.load(os.path.join(split_path, \"tags.npy\"), allow_pickle=True)\n",
    "X_np, Y_np = create_features(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_np, Y_np, random_state=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.90      0.62       609\n",
      "           1       0.54      0.74      0.62       759\n",
      "           2       0.03      0.63      0.05        38\n",
      "           3       0.50      0.80      0.61       464\n",
      "           4       0.12      0.68      0.21       122\n",
      "           5       0.07      0.84      0.14        57\n",
      "           6       0.04      0.56      0.08        52\n",
      "           7       0.56      0.96      0.70       331\n",
      "           8       0.06      0.67      0.12        54\n",
      "           9       0.32      0.99      0.49       169\n",
      "          10       0.25      0.79      0.38       164\n",
      "          11       0.14      0.65      0.23       105\n",
      "          12       0.31      0.81      0.45       170\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.02      0.75      0.03         8\n",
      "          17       0.23      0.83      0.36        84\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.50      0.92      0.65       145\n",
      "          23       0.00      0.50      0.01         2\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.02      1.00      0.04         5\n",
      "          26       0.00      0.00      0.00         0\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.00      0.00      0.00         0\n",
      "          31       0.01      0.75      0.03         4\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.00      0.00      0.00         0\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.48      0.87      0.61        90\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.00      0.00      0.00         0\n",
      "          38       0.03      0.36      0.05        11\n",
      "          39       0.00      0.00      0.00         0\n",
      "          40       0.00      0.00      0.00         0\n",
      "          41       0.00      0.00      0.00         0\n",
      "          42       0.00      0.00      0.00         0\n",
      "          43       0.01      1.00      0.03         2\n",
      "          44       0.00      0.00      0.00         0\n",
      "          45       0.00      0.00      0.00         0\n",
      "          46       0.00      0.00      0.00         0\n",
      "          47       0.00      0.00      0.00         0\n",
      "          48       0.04      0.50      0.08        10\n",
      "          49       0.25      0.64      0.36        42\n",
      "\n",
      "   micro avg       0.16      0.82      0.27      3498\n",
      "   macro avg       0.10      0.36      0.14      3498\n",
      "weighted avg       0.41      0.82      0.53      3498\n",
      " samples avg       0.22      0.41      0.26      3498\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.52      0.54      1242\n",
      "           1       0.54      0.50      0.52      1126\n",
      "           2       0.28      0.29      0.28       862\n",
      "           3       0.49      0.53      0.51       692\n",
      "           4       0.34      0.33      0.33       697\n",
      "           5       0.26      0.27      0.27       613\n",
      "           6       0.30      0.33      0.31       593\n",
      "           7       0.56      0.57      0.56       568\n",
      "           8       0.23      0.24      0.23       547\n",
      "           9       0.36      0.37      0.37       506\n",
      "          10       0.37      0.37      0.37       512\n",
      "          11       0.34      0.35      0.34       467\n",
      "          12       0.43      0.41      0.42       469\n",
      "          13       0.21      0.20      0.20       448\n",
      "          14       0.14      0.18      0.16       382\n",
      "          15       0.19      0.20      0.19       358\n",
      "          16       0.17      0.20      0.19       288\n",
      "          17       0.44      0.43      0.44       310\n",
      "          18       0.15      0.15      0.15       312\n",
      "          19       0.11      0.10      0.11       325\n",
      "          20       0.14      0.14      0.14       325\n",
      "          21       0.07      0.07      0.07       279\n",
      "          22       0.48      0.47      0.48       273\n",
      "          23       0.16      0.19      0.17       231\n",
      "          24       0.16      0.16      0.16       266\n",
      "          25       0.22      0.26      0.24       215\n",
      "          26       0.12      0.11      0.11       241\n",
      "          27       0.18      0.15      0.17       272\n",
      "          28       0.06      0.06      0.06       225\n",
      "          29       0.20      0.18      0.19       283\n",
      "          30       0.08      0.08      0.08       250\n",
      "          31       0.18      0.21      0.20       203\n",
      "          32       0.12      0.13      0.13       183\n",
      "          33       0.10      0.10      0.10       210\n",
      "          34       0.08      0.08      0.08       165\n",
      "          35       0.48      0.46      0.47       168\n",
      "          36       0.07      0.06      0.06       162\n",
      "          37       0.06      0.07      0.06       148\n",
      "          38       0.29      0.24      0.26       179\n",
      "          39       0.09      0.09      0.09       158\n",
      "          40       0.07      0.08      0.08       165\n",
      "          41       0.10      0.09      0.09       161\n",
      "          42       0.10      0.13      0.11       132\n",
      "          43       0.20      0.22      0.21       129\n",
      "          44       0.03      0.03      0.03       135\n",
      "          45       0.03      0.04      0.04       122\n",
      "          46       0.11      0.15      0.13       112\n",
      "          47       0.05      0.06      0.05       122\n",
      "          48       0.29      0.33      0.31       106\n",
      "          49       0.36      0.35      0.36       110\n",
      "\n",
      "   micro avg       0.29      0.30      0.29     17047\n",
      "   macro avg       0.22      0.23      0.22     17047\n",
      "weighted avg       0.30      0.30      0.29     17047\n",
      " samples avg       0.33      0.34      0.29     17047\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71       990\n",
      "           1       0.70      0.67      0.68      1093\n",
      "           2       0.24      0.48      0.32       451\n",
      "           3       0.71      0.69      0.70       761\n",
      "           4       0.42      0.54      0.47       521\n",
      "           5       0.38      0.56      0.45       438\n",
      "           6       0.30      0.55      0.39       360\n",
      "           7       0.73      0.81      0.77       516\n",
      "           8       0.38      0.44      0.41       489\n",
      "           9       0.54      0.80      0.64       351\n",
      "          10       0.56      0.61      0.58       474\n",
      "          11       0.38      0.45      0.41       411\n",
      "          12       0.63      0.73      0.67       385\n",
      "          13       0.16      0.53      0.25       135\n",
      "          14       0.14      0.36      0.21       187\n",
      "          15       0.28      0.59      0.38       172\n",
      "          16       0.56      0.64      0.60       294\n",
      "          17       0.74      0.74      0.74       306\n",
      "          18       0.18      0.44      0.25       128\n",
      "          19       0.08      0.34      0.13        70\n",
      "          20       0.05      0.31      0.08        48\n",
      "          21       0.00      0.03      0.01        30\n",
      "          22       0.71      0.82      0.76       233\n",
      "          23       0.27      0.41      0.32       177\n",
      "          24       0.10      0.38      0.16        68\n",
      "          25       0.63      0.74      0.68       213\n",
      "          26       0.15      0.39      0.22        90\n",
      "          27       0.10      0.39      0.16        61\n",
      "          28       0.00      0.10      0.01        10\n",
      "          29       0.15      0.39      0.21        95\n",
      "          30       0.03      0.20      0.06        45\n",
      "          31       0.58      0.80      0.67       171\n",
      "          32       0.08      0.25      0.12        63\n",
      "          33       0.07      0.39      0.12        33\n",
      "          34       0.05      0.21      0.08        38\n",
      "          35       0.71      0.75      0.73       156\n",
      "          36       0.00      0.00      0.00         8\n",
      "          37       0.03      0.16      0.05        32\n",
      "          38       0.32      0.37      0.34       128\n",
      "          39       0.05      0.25      0.09        32\n",
      "          40       0.05      0.45      0.09        20\n",
      "          41       0.07      0.26      0.11        38\n",
      "          42       0.28      0.69      0.40        70\n",
      "          43       0.57      0.78      0.66       101\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.01      0.12      0.01         8\n",
      "          46       0.27      0.55      0.36        73\n",
      "          47       0.03      0.50      0.05         8\n",
      "          48       0.52      0.53      0.53       116\n",
      "          49       0.61      0.60      0.61       111\n",
      "\n",
      "   micro avg       0.38      0.61      0.47     10809\n",
      "   macro avg       0.31      0.47      0.35     10809\n",
      "weighted avg       0.50      0.61      0.54     10809\n",
      " samples avg       0.46      0.59      0.46     10809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UndefinedMetricWarning)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_pred, Y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
